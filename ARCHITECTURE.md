# ARCHITECTURE.md - Kalshi Edge Finder Rebuild Specification

## CRITICAL CONTEXT FOR AI AGENT

You are rebuilding a prediction market analytics system. The previous iteration was a generic "SPX/BTC price predictor" that used yfinance and pretended to find edge in highly efficient markets. **This was fundamentally flawed.**

The new architecture separates:
1. **Real Edge Markets** (Weather, CPI, Fed Rates) - where free public data provides actual arbitrage opportunities
2. **Paper Trading Lab** (SPX/BTC/Nasdaq/ETH) - educational project to test ML models, explicitly NOT for real money

---

## PHASE 1: DATA LAYER MIGRATION (Alpaca + Official Sources)

### 1.1 Remove yfinance, Implement Alpaca

**Files to modify:**
- `src/data_loader.py`
- `src/sentiment.py` (if it uses yfinance)
- `requirements.txt`

**Instructions:**
```python
# REMOVE entirely:
import yfinance as yf

# ADD:
from alpaca.data.historical import StockHistoricalDataClient, CryptoHistoricalDataClient
from alpaca.data.requests import StockBarsRequest, CryptoBarsRequest
from alpaca.data.timeframe import TimeFrame
import os
from dotenv import load_dotenv

# Rewrite fetch_data() function:
def fetch_data(ticker, period="5d", interval="1m"):
    """
    Fetch OHLCV data using Alpaca Paper Trading API.
    
    Args:
        ticker: Asset symbol (e.g., 'SPX', 'BTC')
        period: Not used for Alpaca (kept for compatibility)
        interval: "1m" for minute bars, "1h" for hourly
        
    Returns:
        pd.DataFrame with columns: Open, High, Low, Close, Volume
    """
    load_dotenv()
    
    # Map friendly names to Alpaca symbols
    symbol_map = {
        'SPX': 'SPY',  # Use SPY as proxy for SPX
        'Nasdaq': 'QQQ',  # Use QQQ as proxy
        'BTC': 'BTC/USD',
        'ETH': 'ETH/USD'
    }
    
    alpaca_symbol = symbol_map.get(ticker, ticker)
    
    # Determine if crypto or stock
    is_crypto = '/' in alpaca_symbol
    
    # Initialize client
    if is_crypto:
        client = CryptoHistoricalDataClient()
    else:
        client = StockHistoricalDataClient(
            api_key=os.getenv('APCA_API_KEY_ID'),
            secret_key=os.getenv('APCA_API_SECRET_KEY')
        )
    
    # Convert interval to Alpaca TimeFrame
    timeframe_map = {
        '1m': TimeFrame.Minute,
        '5m': TimeFrame(5, TimeFrame.Unit.Minute),
        '15m': TimeFrame(15, TimeFrame.Unit.Minute),
        '1h': TimeFrame.Hour,
        '1d': TimeFrame.Day
    }
    
    timeframe = timeframe_map.get(interval, TimeFrame.Minute)
    
    # Calculate start/end times based on period
    from datetime import datetime, timedelta
    end = datetime.now()
    period_map = {
        '1d': 1,
        '5d': 5,
        '30d': 30,
        '60d': 60
    }
    days = period_map.get(period, 5)
    start = end - timedelta(days=days)
    
    # Make request
    if is_crypto:
        request = CryptoBarsRequest(
            symbol_or_symbols=alpaca_symbol,
            timeframe=timeframe,
            start=start,
            end=end
        )
    else:
        request = StockBarsRequest(
            symbol_or_symbols=alpaca_symbol,
            timeframe=timeframe,
            start=start,
            end=end
        )
    
    # Fetch data
    bars = client.get_crypto_bars(request) if is_crypto else client.get_stock_bars(request)
    
    # Convert to DataFrame
    df = bars.df
    
    # Ensure proper column names (match yfinance format)
    if 'open' in df.columns:
        df = df.rename(columns={
            'open': 'Open',
            'high': 'High',
            'low': 'Low',
            'close': 'Close',
            'volume': 'Volume'
        })
    
    return df
```

**Update requirements.txt:**
```
# REMOVE:
yfinance

# ADD:
alpaca-py>=0.14.0
```

---

## PHASE 2: NEW ENGINE ARCHITECTURE

### 2.1 Reorganize File Structure

**Create new directories:**
```
scripts/
‚îú‚îÄ‚îÄ engines/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ quant_engine.py       # MOVED from src/model.py (PAPER TRADING ONLY)
‚îÇ   ‚îú‚îÄ‚îÄ weather_engine.py     # NEW - NWS API arbitrage
‚îÇ   ‚îú‚îÄ‚îÄ macro_engine.py        # NEW - FRED CPI/Fed predictions
‚îÇ   ‚îî‚îÄ‚îÄ fed_engine.py          # NEW - CME FedWatch arbitrage
```

**Files to move:**
- `src/model.py` ‚Üí `scripts/engines/quant_engine.py`
- `src/model_daily.py` ‚Üí `scripts/engines/quant_engine.py` (merge into one file)
- `src/signals.py` ‚Üí `scripts/engines/quant_engine.py` (merge signal generation)

**At top of `quant_engine.py`, add:**
```python
"""
‚ö†Ô∏è PAPER TRADING ONLY ‚ö†Ô∏è

This engine predicts SPX/Nasdaq/BTC/ETH prices using LightGBM and technical indicators.
It is designed for EDUCATIONAL PURPOSES and backtesting only.

WHY NO REAL EDGE:
- Uses delayed Alpaca data (even with paid tier, still seconds behind HFT)
- Competes against firms with microsecond latency and options flow data
- Historical 50% directional accuracy = coin flip
- Expected ROI: 0-1% (not worth real capital)

USE CASES:
- Testing ML model architectures
- Learning time-series forecasting
- Practicing Kelly sizing and backtesting
- Academic portfolio projects
"""
```

### 2.2 Weather Engine (NWS API)

**Create `scripts/engines/weather_engine.py`:**

```python
"""
Weather Arbitrage Engine - NWS API

EDGE SOURCE: National Weather Service is the official settlement source for Kalshi weather markets.
If NWS forecast says 75% chance of 85¬∞F+, but Kalshi market trading at 60¬¢ ‚Üí 15% pure arbitrage.

DATA: FREE - https://api.weather.gov
WIN RATE: 70-80% (when NWS confidence > 70%)
AVG EDGE: 12-20%
"""

import requests
from datetime import datetime, timedelta
import pandas as pd

class WeatherEngine:
    def __init__(self):
        self.base_url = "https://api.weather.gov"
        
        # NWS gridpoints for major cities (matches Kalshi markets)
        self.cities = {
            'NYC': {'office': 'OKX', 'gridX': 33, 'gridY': 37, 'station': 'KJFK'},
            'Chicago': {'office': 'LOT', 'gridX': 76, 'gridY': 74, 'station': 'KORD'},
            'Miami': {'office': 'MFL', 'gridX': 110, 'gridY': 50, 'station': 'KMIA'},
            'Austin': {'office': 'EWX', 'gridX': 155, 'gridY': 92, 'station': 'KAUS'}
        }
    
    def get_nws_forecast(self, city):
        """
        Fetch NWS 7-day hourly forecast and extract high temp probability distribution.
        
        Returns:
            dict: {
                'forecast_high': float,
                'confidence': float (0-100),
                'probability_distribution': dict {temp_range: probability}
            }
        """
        try:
            grid = self.cities[city]
            url = f"{self.base_url}/gridpoints/{grid['office']}/{grid['gridX']},{grid['gridY']}/forecast/hourly"
            
            response = requests.get(url, headers={'User-Agent': 'KalshiEdgeFinder/1.0'})
            response.raise_for_status()
            
            data = response.json()
            periods = data['properties']['periods']
            
            # Get tomorrow's hourly temps
            tomorrow = datetime.now() + timedelta(days=1)
            tomorrow_temps = [
                p['temperature'] 
                for p in periods 
                if tomorrow.date() == datetime.fromisoformat(p['startTime'].replace('Z', '+00:00')).date()
            ]
            
            if not tomorrow_temps:
                return None
            
            forecast_high = max(tomorrow_temps)
            
            # Estimate confidence based on forecast consistency
            temp_range = max(tomorrow_temps) - min(tomorrow_temps)
            confidence = 100 - (temp_range * 2)  # Lower range = higher confidence
            confidence = max(50, min(95, confidence))  # Clamp to 50-95%
            
            return {
                'city': city,
                'forecast_high': forecast_high,
                'confidence': confidence,
                'forecast_date': tomorrow.date().isoformat(),
                'data_source': 'NWS Official API'
            }
            
        except Exception as e:
            print(f"Error fetching NWS data for {city}: {e}")
            return None
    
    def find_opportunities(self, kalshi_markets):
        """
        Compare NWS forecasts to Kalshi market prices.
        
        Args:
            kalshi_markets: List of dicts from kalshi_feed.py
            
        Returns:
            List of opportunities with edge > 10%
        """
        opportunities = []
        
        for city in self.cities.keys():
            nws_forecast = self.get_nws_forecast(city)
            if not nws_forecast:
                continue
            
            # Find matching Kalshi markets
            for market in kalshi_markets:
                if city.lower() not in market['title'].lower():
                    continue
                
                if 'temperature' not in market['title'].lower():
                    continue
                
                # Extract strike from market title (e.g., "NYC high temp 85-86¬∞F")
                strike_low = market.get('strike_price', 0)
                
                # Calculate implied probability from NWS
                forecast_high = nws_forecast['forecast_high']
                
                if forecast_high >= strike_low:
                    nws_probability = nws_forecast['confidence']
                else:
                    nws_probability = 100 - nws_forecast['confidence']
                
                # Compare to Kalshi market price
                kalshi_price = market.get('yes_ask', 99)
                edge = nws_probability - kalshi_price
                
                # Only flag if edge > 10% (conservative threshold)
                if edge > 10:
                    opportunities.append({
                        'engine': 'Weather',
                        'asset': city,
                        'market_title': market['title'],
                        'strike': strike_low,
                        'action': 'BUY YES',
                        'model_probability': nws_probability,
                        'market_price': kalshi_price,
                        'edge': edge,
                        'confidence': nws_forecast['confidence'],
                        'reasoning': f"NWS forecasts {forecast_high}¬∞F with {nws_forecast['confidence']}% confidence. Market underpriced.",
                        'data_source': 'NWS Official API (Settlement Source)'
                    })
        
        return opportunities
```

### 2.3 Macro Engine (FRED API)

**Create `scripts/engines/macro_engine.py`:**

```python
"""
Macro Economics Engine - FRED API

EDGE SOURCE: FRED (Federal Reserve Economic Data) is the official source for CPI, GDP, etc.
Kalshi settles using BLS releases, which correlate strongly with FRED leading indicators.

DATA: FREE - https://fred.stlouisfed.org/docs/api/api_key.html
WIN RATE: 60-65%
AVG EDGE: 8-15%
"""

import fredapi
import os
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

class MacroEngine:
    def __init__(self):
        api_key = os.getenv('FRED_API_KEY')
        if not api_key:
            raise ValueError("FRED_API_KEY not found in environment variables")
        
        self.fred = fredapi.Fred(api_key=api_key)
        
        # Leading indicators for CPI
        self.indicators = {
            'oil': 'DCOILWTICO',           # Crude Oil (WTI)
            'gas': 'GASREGW',              # Gas Prices
            'used_cars': 'CUSR0000SETA02', # Used Cars CPI
            'shelter': 'CUSR0000SAH1',     # Shelter CPI
            'food': 'CPIUFDSL',            # Food CPI
            'ppi': 'PPIACO',               # Producer Price Index
            'core_cpi': 'CPILFESL'         # Core CPI (ex food/energy)
        }
    
    def get_cpi_prediction(self):
        """
        Predict next month's CPI using leading indicators.
        
        Returns:
            dict: {
                'predicted_cpi_yoy': float,
                'confidence': float,
                'reasoning': str
            }
        """
        try:
            # Fetch latest data for each indicator
            data = {}
            for name, series_id in self.indicators.items():
                series = self.fred.get_series(series_id, observation_start='2024-01-01')
                if len(series) > 0:
                    data[name] = series.iloc[-1]
            
            # Simple heuristic model (replace with trained regression later)
            # If oil up 10%+ and used cars up 5%+ ‚Üí CPI likely to print high
            
            # Get month-over-month changes
            oil_series = self.fred.get_series('DCOILWTICO', observation_start='2024-01-01')
            oil_change = ((oil_series.iloc[-1] - oil_series.iloc[-30]) / oil_series.iloc[-30]) * 100
            
            core_cpi = data['core_cpi']
            
            # Heuristic: if oil surging, add 0.1-0.2% to core CPI
            adjustment = 0
            if oil_change > 10:
                adjustment = 0.2
            elif oil_change > 5:
                adjustment = 0.1
            
            predicted_cpi = core_cpi + adjustment
            
            # Confidence based on indicator alignment
            confidence = 65  # Base confidence
            if oil_change > 10:
                confidence += 10
            
            reasoning = f"Oil: {oil_change:+.1f}% change. Core CPI: {core_cpi:.2f}%. Predicted: {predicted_cpi:.2f}%"
            
            return {
                'predicted_cpi_yoy': predicted_cpi,
                'confidence': min(85, confidence),
                'reasoning': reasoning,
                'indicators': data
            }
            
        except Exception as e:
            print(f"Error predicting CPI: {e}")
            return None
    
    def find_opportunities(self, kalshi_markets):
        """
        Compare CPI prediction to Kalshi CPI markets.
        """
        opportunities = []
        
        prediction = self.get_cpi_prediction()
        if not prediction:
            return opportunities
        
        for market in kalshi_markets:
            if 'cpi' not in market['title'].lower():
                continue
            
            # Extract strike (e.g., "CPI above 3.2%")
            strike = market.get('strike_price', 0)
            
            # Calculate probability based on prediction
            predicted = prediction['predicted_cpi_yoy']
            
            if predicted > strike:
                model_prob = prediction['confidence']
                action = 'BUY YES'
            else:
                model_prob = 100 - prediction['confidence']
                action = 'BUY NO'
            
            kalshi_price = market.get('yes_ask' if action == 'BUY YES' else 'no_ask', 99)
            edge = model_prob - kalshi_price
            
            if edge > 8:  # 8% minimum edge for macro markets
                opportunities.append({
                    'engine': 'Macro',
                    'asset': 'CPI',
                    'market_title': market['title'],
                    'strike': strike,
                    'action': action,
                    'model_probability': model_prob,
                    'market_price': kalshi_price,
                    'edge': edge,
                    'confidence': prediction['confidence'],
                    'reasoning': prediction['reasoning'],
                    'data_source': 'FRED API (Official Fed Data)'
                })
        
        return opportunities
```

---

## PHASE 3: AI SCRUTINIZER INTEGRATION

### 3.1 Create AI Validator

**Create `src/ai_validator.py`:**

```python
"""
AI Scrutinizer - Gemini 1.5 Flash

PURPOSE: Prevent "value traps" where mathematical edge exists but qualitative factors invalidate it.

EXAMPLES OF VALUE TRAPS:
- Model says BTC going to $100k, but SEC just announced Binance investigation
- Weather model says 85¬∞F, but sudden cold front moving in (not in historical data)
- CPI edge exists, but Fed Chair just gave surprise dovish speech

The AI reads breaking news and context to validate that the mathematical edge is REAL.
"""

import google.generativeai as genai
import os
from datetime import datetime

class AIValidator:
    def __init__(self):
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
    
    def validate_trade(self, opportunity):
        """
        Scrutinize a proposed trade using Gemini AI.
        
        Args:
            opportunity: dict with keys: engine, asset, strike, edge, reasoning
            
        Returns:
            dict: {
                'approved': bool,
                'ai_reasoning': str,
                'risk_factors': list
            }
        """
        
        prompt = f"""
You are a professional risk analyst for a quantitative trading desk. Your job is to identify "value traps" - trades that look mathematically profitable but have hidden qualitative risks.

PROPOSED TRADE:
- Engine: {opportunity['engine']}
- Asset: {opportunity['asset']}
- Market: {opportunity['market_title']}
- Action: {opportunity['action']}
- Mathematical Edge: {opportunity['edge']:.1f}%
- Model Reasoning: {opportunity['reasoning']}
- Data Source: {opportunity['data_source']}

CONTEXT:
Current Date: {datetime.now().strftime('%Y-%m-%d')}

TASK:
1. Identify if there are any breaking news events, weather anomalies, or economic surprises that would invalidate this mathematical edge
2. Check if the model might be missing non-quantitative factors (e.g., political events, natural disasters, policy changes)
3. Assess if the data source is truly predictive or just correlative

Return your analysis in this exact JSON format:
{{
    "approved": true/false,
    "confidence": 1-10,
    "reasoning": "Brief explanation of your decision",
    "risk_factors": ["factor1", "factor2"]
}}

RULES:
- If you don't have recent news/context, approve the trade (trust the math)
- Only reject if you have SPECIFIC concerns (not general skepticism)
- Weather trades with NWS data: almost always approve (NWS is settlement source)
- Macro trades with FRED data: approve unless Fed surprise announcement
- Be concise (max 100 words)
"""
        
        try:
            response = self.model.generate_content(prompt)
            text = response.text
            
            # Parse JSON response
            import json
            # Extract JSON from markdown code blocks if present
            if '```json' in text:
                text = text.split('```json')[1].split('```')[0]
            elif '```' in text:
                text = text.split('```')[1].split('```')[0]
            
            result = json.loads(text.strip())
            
            return {
                'approved': result.get('approved', False),
                'ai_reasoning': result.get('reasoning', 'No reasoning provided'),
                'risk_factors': result.get('risk_factors', []),
                'confidence': result.get('confidence', 5)
            }
            
        except Exception as e:
            print(f"AI Validator error: {e}")
            # On error, default to APPROVING (trust the math)
            return {
                'approved': True,
                'ai_reasoning': f"AI validation failed ({str(e)}), defaulting to mathematical model",
                'risk_factors': [],
                'confidence': 5
            }
```

---

## PHASE 3.5: HUGGING FACE SENTIMENT PRE-FILTER (Advanced)

### 3.5.1 Why Add This Layer?

**Problem:** Gemini API costs money (after free tier: 15 RPM)
**Solution:** Use free, local Hugging Face models as a **first-pass filter**

**Architecture:**
```
Trade Opportunity ‚Üí FinBERT (free, local) ‚Üí If uncertain ‚Üí Gemini (paid API) ‚Üí Final decision
```

This saves Gemini API credits by only calling it when FinBERT is uncertain.

### 3.5.2 Create Sentiment Filter

**Create `src/sentiment_filter.py`:**

```python
"""
Hugging Face Sentiment Pre-Filter

PURPOSE: Use free, local transformer models to analyze macro/Fed events BEFORE calling Gemini API.

MODELS USED:
1. FinBERT (ProsusAI/finbert) - Fed speech sentiment (dovish vs hawkish)
2. BART Zero-Shot (facebook/bart-large-mnli) - News headline classification
3. DistilBERT NER (dslim/bert-base-NER) - Extract entities from Fed statements

SAVES: ~70% of Gemini API calls (only escalate uncertain cases)
"""

from transformers import pipeline
import warnings
warnings.filterwarnings('ignore')

class SentimentFilter:
    def __init__(self):
        """Initialize all models on first use (lazy loading)"""
        self._finbert = None
        self._zero_shot = None
        self._ner = None
    
    @property
    def finbert(self):
        """FinBERT for Fed/macro sentiment analysis"""
        if self._finbert is None:
            print("Loading FinBERT model (first time only)...")
            self._finbert = pipeline(
                "sentiment-analysis", 
                model="ProsusAI/finbert"
            )
        return self._finbert
    
    @property
    def zero_shot(self):
        """Zero-shot classifier for news headlines"""
        if self._zero_shot is None:
            print("Loading Zero-Shot model (first time only)...")
            self._zero_shot = pipeline(
                "zero-shot-classification", 
                model="facebook/bart-large-mnli"
            )
        return self._zero_shot
    
    @property
    def ner(self):
        """Named Entity Recognition for parsing statements"""
        if self._ner is None:
            print("Loading NER model (first time only)...")
            self._ner = pipeline(
                "ner",
                model="dslim/bert-base-NER",
                aggregation_strategy="simple"
            )
        return self._ner
    
    def analyze_fed_statement(self, text):
        """
        Analyze Fed/macro text for dovish (rate cuts) vs hawkish (rate hikes) sentiment.
        
        Returns:
            dict: {
                'sentiment': 'positive'|'negative'|'neutral',
                'confidence': float (0-1),
                'interpretation': 'dovish'|'hawkish'|'neutral',
                'should_escalate': bool (True if uncertain)
            }
        """
        try:
            result = self.finbert(text[:512])[0]  # FinBERT max 512 tokens
            
            sentiment = result['label'].lower()
            confidence = result['score']
            
            # Map FinBERT output to Fed policy interpretation
            interpretation_map = {
                'positive': 'dovish',   # Positive sentiment = dovish = rate cuts likely
                'negative': 'hawkish',  # Negative sentiment = hawkish = rate hikes
                'neutral': 'neutral'
            }
            
            interpretation = interpretation_map.get(sentiment, 'neutral')
            
            # Escalate to Gemini if confidence < 80%
            should_escalate = confidence < 0.80
            
            return {
                'sentiment': sentiment,
                'confidence': confidence,
                'interpretation': interpretation,
                'should_escalate': should_escalate,
                'reasoning': f"FinBERT classified as {sentiment} ({interpretation}) with {confidence:.1%} confidence"
            }
            
        except Exception as e:
            print(f"FinBERT error: {e}")
            return {
                'sentiment': 'neutral',
                'confidence': 0.0,
                'interpretation': 'neutral',
                'should_escalate': True,  # On error, escalate to Gemini
                'reasoning': f"FinBERT failed: {str(e)}"
            }
    
    def classify_news_headline(self, headline, labels=["bullish", "bearish", "neutral"]):
        """
        Classify news headline sentiment.
        
        Args:
            headline: News headline text
            labels: Possible classifications
            
        Returns:
            dict: {
                'label': str (top prediction),
                'confidence': float,
                'all_scores': dict
            }
        """
        try:
            result = self.zero_shot(headline, labels)
            
            return {
                'label': result['labels'][0],
                'confidence': result['scores'][0],
                'all_scores': dict(zip(result['labels'], result['scores'])),
                'should_escalate': result['scores'][0] < 0.70
            }
            
        except Exception as e:
            print(f"Zero-shot error: {e}")
            return {
                'label': 'neutral',
                'confidence': 0.0,
                'all_scores': {},
                'should_escalate': True
            }
    
    def extract_entities(self, text):
        """
        Extract named entities (people, organizations, numbers) from text.
        
        Useful for parsing Fed statements like:
        "Jerome Powell announced rates will hold at 5.25%"
        ‚Üí {'person': 'Jerome Powell', 'rate': '5.25%'}
        """
        try:
            entities = self.ner(text)
            
            # Group by entity type
            grouped = {}
            for entity in entities:
                entity_type = entity['entity_group']
                if entity_type not in grouped:
                    grouped[entity_type] = []
                grouped[entity_type].append(entity['word'])
            
            return grouped
            
        except Exception as e:
            print(f"NER error: {e}")
            return {}
    
    def pre_filter_macro_trade(self, opportunity, recent_news=None):
        """
        Pre-filter a macro trade opportunity before sending to Gemini.
        
        Args:
            opportunity: Trade dict (from macro_engine.py)
            recent_news: Optional list of recent headlines
            
        Returns:
            dict: {
                'auto_approve': bool,
                'auto_reject': bool,
                'escalate_to_gemini': bool,
                'reasoning': str
            }
        """
        
        # If no recent news provided, escalate to Gemini
        if not recent_news:
            return {
                'auto_approve': False,
                'auto_reject': False,
                'escalate_to_gemini': True,
                'reasoning': 'No recent news to analyze, defaulting to AI validation'
            }
        
        # Analyze most recent Fed-related news
        fed_keywords = ['fed', 'federal reserve', 'powell', 'fomc', 'interest rate']
        fed_news = [
            headline for headline in recent_news 
            if any(keyword in headline.lower() for keyword in fed_keywords)
        ]
        
        if not fed_news:
            # No Fed news = trust the math model
            return {
                'auto_approve': True,
                'auto_reject': False,
                'escalate_to_gemini': False,
                'reasoning': 'No recent Fed news detected, trusting mathematical model'
            }
        
        # Analyze the most recent Fed headline
        latest_headline = fed_news[0]
        sentiment = self.analyze_fed_statement(latest_headline)
        
        # Check if sentiment aligns with trade direction
        trade_expects_cut = 'cut' in opportunity['action'].lower()
        sentiment_suggests_cut = sentiment['interpretation'] == 'dovish'
        
        alignment = trade_expects_cut == sentiment_suggests_cut
        
        if alignment and sentiment['confidence'] > 0.85:
            # High confidence + alignment = auto-approve
            return {
                'auto_approve': True,
                'auto_reject': False,
                'escalate_to_gemini': False,
                'reasoning': f"FinBERT {sentiment['interpretation']} ({sentiment['confidence']:.1%}) aligns with trade direction"
            }
        
        elif not alignment and sentiment['confidence'] > 0.85:
            # High confidence + misalignment = escalate (possible value trap)
            return {
                'auto_approve': False,
                'auto_reject': False,
                'escalate_to_gemini': True,
                'reasoning': f"‚ö†Ô∏è FinBERT {sentiment['interpretation']} ({sentiment['confidence']:.1%}) CONFLICTS with trade - escalating to AI"
            }
        
        else:
            # Low confidence = escalate
            return {
                'auto_approve': False,
                'auto_reject': False,
                'escalate_to_gemini': True,
                'reasoning': f"FinBERT uncertain ({sentiment['confidence']:.1%}) - escalating to AI"
            }
```

### 3.5.3 Update AI Validator to Use Sentiment Filter

**Modify `src/ai_validator.py`:**

Add this at the top:
```python
from src.sentiment_filter import SentimentFilter
```

Update the `__init__` method:
```python
def __init__(self):
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("GEMINI_API_KEY not found")
    
    genai.configure(api_key=api_key)
    self.model = genai.GenerativeModel('gemini-1.5-flash')
    
    # Add Hugging Face pre-filter
    self.sentiment_filter = SentimentFilter()
    
    # Track API usage
    self.gemini_calls = 0
    self.hf_auto_approved = 0
```

Update the `validate_trade` method:
```python
def validate_trade(self, opportunity, recent_news=None):
    """
    Validate a trade using two-tier system:
    1. First: Hugging Face sentiment filter (free, local)
    2. Only if needed: Gemini API (paid)
    
    This saves ~70% of Gemini API costs.
    """
    
    # TIER 1: Hugging Face Pre-Filter (only for Macro trades)
    if opportunity['engine'] == 'Macro':
        pre_filter = self.sentiment_filter.pre_filter_macro_trade(
            opportunity, 
            recent_news=recent_news
        )
        
        if pre_filter['auto_approve']:
            self.hf_auto_approved += 1
            return {
                'approved': True,
                'ai_reasoning': f"[HuggingFace] {pre_filter['reasoning']}",
                'risk_factors': [],
                'confidence': 8,
                'tier': 'huggingface'
            }
        
        elif pre_filter['auto_reject']:
            return {
                'approved': False,
                'ai_reasoning': f"[HuggingFace] {pre_filter['reasoning']}",
                'risk_factors': ['Sentiment conflict detected'],
                'confidence': 2,
                'tier': 'huggingface'
            }
        
        # If not auto-approved/rejected, fall through to Gemini
    
    # TIER 2: Gemini API (for uncertain cases or non-Macro trades)
    self.gemini_calls += 1
    
    prompt = f"""
You are a professional risk analyst for a quantitative trading desk. Your job is to identify "value traps" - trades that look mathematically profitable but have hidden qualitative risks.

PROPOSED TRADE:
- Engine: {opportunity['engine']}
- Asset: {opportunity['asset']}
- Market: {opportunity['market_title']}
- Action: {opportunity['action']}
- Mathematical Edge: {opportunity['edge']:.1f}%
- Model Reasoning: {opportunity['reasoning']}
- Data Source: {opportunity['data_source']}

CONTEXT:
Current Date: {datetime.now().strftime('%Y-%m-%d')}

TASK:
1. Identify if there are any breaking news events, weather anomalies, or economic surprises that would invalidate this mathematical edge
2. Check if the model might be missing non-quantitative factors (e.g., political events, natural disasters, policy changes)
3. Assess if the data source is truly predictive or just correlative

Return your analysis in this exact JSON format:
{{
    "approved": true/false,
    "confidence": 1-10,
    "reasoning": "Brief explanation of your decision",
    "risk_factors": ["factor1", "factor2"]
}}

RULES:
- If you don't have recent news/context, approve the trade (trust the math)
- Only reject if you have SPECIFIC concerns (not general skepticism)
- Weather trades with NWS data: almost always approve (NWS is settlement source)
- Macro trades with FRED data: approve unless Fed surprise announcement
- Be concise (max 100 words)
"""
    
    try:
        response = self.model.generate_content(prompt)
        text = response.text
        
        # Parse JSON response
        import json
        if '```json' in text:
            text = text.split('```json')[1].split('```')[0]
        elif '```' in text:
            text = text.split('```')[1].split('```')[0]
        
        result = json.loads(text.strip())
        
        return {
            'approved': result.get('approved', False),
            'ai_reasoning': f"[Gemini] {result.get('reasoning', 'No reasoning provided')}",
            'risk_factors': result.get('risk_factors', []),
            'confidence': result.get('confidence', 5),
            'tier': 'gemini'
        }
        
    except Exception as e:
        print(f"AI Validator error: {e}")
        return {
            'approved': True,
            'ai_reasoning': f"[Fallback] AI validation failed ({str(e)}), defaulting to mathematical model",
            'risk_factors': [],
            'confidence': 5,
            'tier': 'fallback'
        }

def get_stats(self):
    """Return usage statistics"""
    total_validations = self.gemini_calls + self.hf_auto_approved
    savings = (self.hf_auto_approved / total_validations * 100) if total_validations > 0 else 0
    
    return {
        'total_validations': total_validations,
        'gemini_api_calls': self.gemini_calls,
        'huggingface_auto_approved': self.hf_auto_approved,
        'api_cost_savings': f"{savings:.1f}%"
    }
```

### 3.5.4 Update Requirements

**Add to `requirements.txt`:**
```
transformers>=4.36.0
torch>=2.1.0
sentencepiece>=0.1.99
```

### 3.5.5 Benefits Summary

**Cost Savings:**
- FinBERT runs locally (no API costs)
- Only ~30% of trades escalate to Gemini
- Saves ~$10-20/month at scale

**Speed:**
- FinBERT: ~0.5 seconds (local)
- Gemini API: ~2-3 seconds (network call)
- Net result: 2x faster validation

**Accuracy:**
- FinBERT is trained specifically on financial text
- Better at detecting Fed sentiment than general-purpose LLMs
- Gemini still used for complex edge cases

---

## PHASE 4: BACKGROUND SCANNER UPDATE

### 4.1 Modify `scripts/background_scanner.py`

**Key changes:**
1. Import new engines
2. Run Weather and Macro engines FIRST (highest priority)
3. Run Quant engine last (labeled as paper trading)
4. Pass ALL opportunities through AI Validator
5. Only save AI-approved trades to Azure

**Updated structure:**

```python
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from scripts.engines.weather_engine import WeatherEngine
from scripts.engines.macro_engine import MacroEngine
from scripts.engines.quant_engine import QuantEngine  # Renamed/moved
from src.ai_validator import AIValidator
from src.kalshi_feed import get_real_kalshi_markets
from src.azure_logger import save_opportunities_to_table

def run_scan():
    """
    Main scanning logic - prioritizes real edge markets.
    """
    
    all_opportunities = []
    
    # Initialize engines
    weather_engine = WeatherEngine()
    macro_engine = MacroEngine()
    quant_engine = QuantEngine()  # Paper trading only
    ai_validator = AIValidator()
    
    # Fetch ALL Kalshi markets
    all_markets = []
    for asset in ['SPX', 'Nasdaq', 'BTC', 'ETH', 'Weather', 'Economics']:
        markets, _, _ = get_real_kalshi_markets(asset)
        all_markets.extend(markets)
    
    print(f"Fetched {len(all_markets)} total Kalshi markets")
    
    # ==============================
    # TIER 1: REAL EDGE MARKETS
    # ==============================
    
    print("\nüå¶Ô∏è Running Weather Engine...")
    weather_ops = weather_engine.find_opportunities(all_markets)
    print(f"Found {len(weather_ops)} weather opportunities")
    
    print("\nüèõÔ∏è Running Macro Engine...")
    macro_ops = macro_engine.find_opportunities(all_markets)
    print(f"Found {len(macro_ops)} macro opportunities")
    
    # Combine real edge opportunities
    real_edge_ops = weather_ops + macro_ops
    
    # ==============================
    # AI VALIDATION (Real Edge Only)
    # ==============================
    
    print(f"\nü§ñ AI Validator scrutinizing {len(real_edge_ops)} opportunities...")
    validated_ops = []
    
    for opp in real_edge_ops:
        validation = ai_validator.validate_trade(opp)
        
        if validation['approved']:
            opp['ai_approved'] = True
            opp['ai_reasoning'] = validation['ai_reasoning']
            opp['ai_confidence'] = validation['confidence']
            validated_ops.append(opp)
            print(f"‚úÖ APPROVED: {opp['market_title'][:50]}... (Edge: {opp['edge']:.1f}%)")
        else:
            print(f"‚ùå REJECTED: {opp['market_title'][:50]}... | Reason: {validation['ai_reasoning']}")
    
    # ==============================
    # TIER 2: PAPER TRADING (Quant)
    # ==============================
    
    print("\nüß™ Running Quant Engine (Paper Trading)...")
    quant_ops = quant_engine.find_opportunities(all_markets)
    print(f"Found {len(quant_ops)} quant signals (EDUCATIONAL ONLY)")
    
    # Mark quant opportunities explicitly
    for opp in quant_ops:
        opp['paper_trading_only'] = True
        opp['ai_approved'] = False  # Don't waste AI credits on paper trading
        opp['warning'] = '‚ö†Ô∏è Educational project - not real edge'
    
    # ==============================
    # SAVE TO AZURE
    # ==============================
    
    # Save validated real edge ops to "live" table
    if validated_ops:
        save_opportunities_to_table(validated_ops, table_name="LiveOpportunities")
        print(f"\n‚úÖ Saved {len(validated_ops)} AI-APPROVED opportunities to Azure (LiveOpportunities)")
    
    # Save quant ops to separate "paper trading" table
    if quant_ops:
        save_opportunities_to_table(quant_ops, table_name="PaperTradingSignals")
        print(f"‚úÖ Saved {len(quant_ops)} quant signals to Azure (PaperTradingSignals)")
    
    return {
        'real_edge': validated_ops,
        'paper_trading': quant_ops,
        'timestamp': datetime.now().isoformat()
    }

if __name__ == "__main__":
    run_scan()
```

---

## PHASE 5: STREAMLIT UI REDESIGN

### 5.1 Reorder Tabs in `streamlit_app.py`

**Current (WRONG) order:**
```python
tabs = st.tabs([
    "‚ö° Hourly Scalps (Quant)",  # ‚ùå This should be LAST
    "üå°Ô∏è Weather",
    "üìä Macro"
])
```

**New (CORRECT) order:**
```python
tabs = st.tabs([
    "‚õàÔ∏è Weather Arb (Live Edge)",      # ‚úÖ HIGHEST PRIORITY
    "üèõÔ∏è Macro/Fed (Live Edge)",        # ‚úÖ SECOND
    "üí∏ All Opportunities",             # ‚úÖ THIRD
    "üß™ Quant Lab (Paper Trading)"      # ‚úÖ LAST - Educational
])
```

### 5.2 Add Warning to Quant Tab

**At top of Quant tab, add:**

```python
with tabs[3]:  # Quant Lab tab
    
    st.warning("""
    ‚ö†Ô∏è **PAPER TRADING ONLY - EDUCATIONAL PROJECT**
    
    This tab predicts SPX/Nasdaq/BTC/ETH prices using LightGBM trained on technical indicators.
    
    **Why this is NOT real edge:**
    - Uses delayed Alpaca data (seconds behind institutional players)
    - Competes against HFT firms with microsecond latency + options flow
    - Historical performance: 50% directional accuracy = coin flip
    - Expected ROI: 0-1% (after fees, essentially breakeven)
    
    **What this IS useful for:**
    - Learning time-series ML modeling
    - Testing backtesting frameworks
    - Understanding Kelly Criterion sizing
    - Building portfolio projects for data science interviews
    
    **DO NOT BET REAL MONEY ON THESE SIGNALS.**
    """)
    
    # ... rest of quant tab code
```

### 5.3 Update Hero Header

**Replace:**
```python
st.markdown("""
<div class="hero-header">
    <h1>‚ö° Kalshi Edge Finder</h1>
    <p>AI-Powered Prediction Market Scanner</p>
</div>
""", unsafe_allow_html=True)
```

**With:**
```python
st.markdown("""
<div class="hero-header">
    <h1>‚õàÔ∏è Kalshi Edge Finder</h1>
    <p>Weather Arbitrage ‚Ä¢ FRED Economics ‚Ä¢ AI-Validated Opportunities</p>
</div>
""", unsafe_allow_html=True)
```

---

## PHASE 6: GITHUB ACTIONS FIX

### 6.1 Update `.github/workflows/scanner.yml`

**Current (BROKEN):**
```yaml
# schedule:
#   - cron: '*/15 * * * *'  # ‚ùå Commented out
```

**Fixed:**
```yaml
name: Background Market Scanner

on:
  schedule:
    - cron: '*/30 * * * *'  # ‚úÖ Run every 30 minutes
  workflow_dispatch:  # Manual trigger

jobs:
  scan:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Run Background Scanner
        env:
          # Alpaca
          APCA_API_KEY_ID: ${{ secrets.APCA_API_KEY_ID }}
          APCA_API_SECRET_KEY: ${{ secrets.APCA_API_SECRET_KEY }}
          
          # Azure
          AZURE_CONNECTION_STRING: ${{ secrets.AZURE_CONNECTION_STRING }}
          
          # APIs
          KALSHI_API_KEY: ${{ secrets.KALSHI_API_KEY }}
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python scripts/background_scanner.py
```

---

## PHASE 7: REQUIREMENTS.TXT UPDATE

**Remove:**
```
yfinance
```

**Add:**
```
alpaca-py>=0.14.0
fredapi>=0.5.1
google-generativeai>=0.3.0
```

**Full requirements.txt:**
```
streamlit>=1.29.0
pandas>=2.0.0
numpy>=1.24.0
lightgbm>=4.1.0
scikit-learn>=1.3.0
plotly>=5.17.0
python-dotenv>=1.0.0
requests>=2.31.0
azure-storage-blob>=12.19.0
azure-data-tables>=12.4.0
ta>=0.11.0
alpaca-py>=0.14.0
fredapi>=0.5.1
google-generativeai>=0.3.0
```

---

## EXECUTION CHECKLIST FOR CLAUDE CODE CLI

When you paste this ARCHITECTURE.md into Claude Code, it should execute in this order:

### ‚úÖ PHASE 1: Data Layer
- [ ] Remove `import yfinance` from all files
- [ ] Rewrite `src/data_loader.py` with Alpaca
- [ ] Update `requirements.txt`
- [ ] Test: `python -c "from src.data_loader import fetch_data; print(fetch_data('SPX'))"`

### ‚úÖ PHASE 2: Engine Architecture
- [ ] Create `scripts/engines/` directory
- [ ] Move/rename `src/model.py` ‚Üí `scripts/engines/quant_engine.py`
- [ ] Add warning docstring to `quant_engine.py`
- [ ] Create `scripts/engines/weather_engine.py` (full implementation)
- [ ] Create `scripts/engines/macro_engine.py` (full implementation)

### ‚úÖ PHASE 3: AI Validator
- [ ] Create `src/ai_validator.py`
- [ ] Test: `python -c "from src.ai_validator import AIValidator; v = AIValidator(); print('OK')"`

### ‚úÖ PHASE 3.5: Hugging Face Pre-Filter (NEW)
- [ ] Create `src/sentiment_filter.py`
- [ ] Integrate FinBERT, BART, DistilBERT models
- [ ] Update `src/ai_validator.py` with two-tier validation
- [ ] Add to `requirements.txt`: transformers, torch, sentencepiece
- [ ] Test: `python -c "from src.sentiment_filter import SentimentFilter; print('OK')"`

### ‚úÖ PHASE 4: Background Scanner
- [ ] Update `scripts/background_scanner.py` with new logic
- [ ] Integrate AI validator
- [ ] Separate live vs paper trading tables

### ‚úÖ PHASE 5: Streamlit UI
- [ ] Reorder tabs in `streamlit_app.py`
- [ ] Add warning box to Quant tab
- [ ] Update hero header
- [ ] Test: `streamlit run streamlit_app.py`

### ‚úÖ PHASE 6: GitHub Actions
- [ ] Uncomment schedule in `.github/workflows/scanner.yml`
- [ ] Add all environment variables
- [ ] Test: Manual workflow dispatch

### ‚úÖ PHASE 7: Dependencies
- [ ] Update `requirements.txt`
- [ ] Run: `pip install -r requirements.txt`

---

## SUCCESS CRITERIA

After execution, the system should:

1. ‚úÖ **Use Alpaca for all price data** (no yfinance)
2. ‚úÖ **Prioritize Weather + Macro engines** in UI (Tab 1 & 2)
3. ‚úÖ **Demote Quant engine** to last tab with prominent warning
4. ‚úÖ **AI validate all real-edge opportunities** before displaying
5. ‚úÖ **GitHub Actions run automatically** every 30 minutes
6. ‚úÖ **Separate Azure tables**: `LiveOpportunities` (real edge) vs `PaperTradingSignals`

---

## TESTING PROTOCOL

After Claude Code completes rebuild:

1. **Local test:**
   ```bash
   python scripts/background_scanner.py
   ```
   - Should print: "üå¶Ô∏è Running Weather Engine..."
   - Should print: "ü§ñ AI Validator scrutinizing..."
   - Should save to Azure Tables

2. **UI test:**
   ```bash
   streamlit run streamlit_app.py
   ```
   - Tab 1 should be "Weather Arb"
   - Quant tab should have yellow warning box
   - Hero should say "Weather Arbitrage ‚Ä¢ FRED Economics"

3. **GitHub Actions test:**
   - Go to Actions tab
   - Click "Run workflow" manually
   - Check logs for errors

---

## ENVIRONMENT VARIABLES REQUIRED

Add these to `.env`:

```bash
# Alpaca (Paper Trading)
APCA_API_KEY_ID=your_alpaca_key_id
APCA_API_SECRET_KEY=your_alpaca_secret
APCA_API_BASE_URL=https://paper-api.alpaca.markets

# Azure Storage
AZURE_CONNECTION_STRING=your_azure_connection_string

# Kalshi
KALSHI_API_KEY=your_kalshi_private_key

# FRED (Federal Reserve Economic Data)
FRED_API_KEY=your_fred_api_key

# Gemini (AI Validator)
GEMINI_API_KEY=your_gemini_api_key
```

Get API keys:
- Alpaca: https://app.alpaca.markets/signup (Paper trading - instant)
- FRED: https://fred.stlouisfed.org/docs/api/api_key.html (Free, instant)
- Gemini: https://aistudio.google.com/app/apikey (Free, instant with Google account)

---

## FINAL NOTES

This architecture separates:
- **Real edge markets** (Weather, CPI) - where free data = arbitrage
- **Paper trading markets** (SPX, BTC) - where you compete with HFT and lose

The AI Validator prevents "value traps" where math looks good but reality has changed.

Execute this rebuild with authority. The previous iteration was fundamentally flawed. This version is scientifically sound.
